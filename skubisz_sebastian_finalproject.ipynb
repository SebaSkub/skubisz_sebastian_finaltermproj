{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dfaa20-2c0c-4e6c-8d26-7bc8873b49f1",
   "metadata": {},
   "source": [
    "**CS634 Final Project** <br> \n",
    "*Performance Evaluation of Classification Models for Water Quality Prediction Using SMOTE and Cross-Validation*<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fd40f-6925-47f9-8850-5bf92f1c4ef4",
   "metadata": {},
   "source": [
    "Student: Sebastian Skubisz<br>\n",
    "UCID: ss365<br>\n",
    "Instructor: Dr. yasser<br>\n",
    "**NOTICE: It produces large output, please it run it locally, using the restart kernel and run all cells**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53e7a0-d914-4f38-8c10-a74a0572af76",
   "metadata": {},
   "source": [
    "This project aims to preprocess and analyze a water quality dataset to predict whether water is safe to drink using machine learning models such as K-Nearest Neighbors (KNN), Random Forest (RF), and LSTM. The workflow includes data cleaning, visualizing feature distributions, and applying SMOTE to balance the classes. \n",
    "\n",
    "Hyperparameters for KNN and RF are tuned via grid search, and the models are evaluated using 10-fold cross-validation. The best-performing models are then trained on the entire dataset and tested on a separate test set. Their performance is compared using ROC curves and AUC scores, providing a comprehensive assessment of their prediction capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654160f-6fd9-468b-a4e8-d8f7ad407cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89620036-b9e4-489d-a1d2-6c606d2e4bd8",
   "metadata": {},
   "source": [
    "This code imports essential libraries for data manipulation and visualization. pandas is used for handling structured data, while numpy supports numerical operations. The matplotlib library is configured with a non-GUI backend to prevent errors in non-interactive environments, and seaborn enhances visualizations. Additionally, warnings are suppressed to maintain a clean output during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb8b5e-f6bc-4b89-a877-07296bce9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "from imblearn.over_sampling import SMOTE  # SMOTE integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa501e6-7dcb-4029-8de2-e576d7bb992c",
   "metadata": {},
   "source": [
    "This code imports several libraries for machine learning and evaluation. It uses Counter to count data occurrences. sklearn provides tools for classification models like KNeighbors and Random Forest, as well as metrics and cross-validation methods such as GridSearchCV, StratifiedKFold, and train_test_split. Metrics like confusion matrix, ROC AUC, and Brier score are also imported for evaluating models.\n",
    "\n",
    "It also includes Keras for building deep learning models, like Sequential, Dense, and LSTM layers. Lastly, SMOTE from imblearn is imported to handle class imbalance by oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a22270-7200-409b-b7c7-553732e362a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading Data And Preprocessing\n",
    "\n",
    "# For quick tests, you can switch this to 'waterQuality1_sample.csv'\n",
    "diab = pd.read_csv('waterQuality1.csv')\n",
    "print(\"\\nLoaded dataset with shape:\", diab.shape)\n",
    "print(\"\\nDataset info:\")\n",
    "diab.info()\n",
    "\n",
    "feature_cols = [\n",
    "    'aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium',\n",
    "    'chloramine', 'chromium', 'copper', 'flouride', 'bacteria',\n",
    "    'viruses', 'lead', 'nitrates', 'nitrites', 'mercury',\n",
    "    'perchlorate', 'radium', 'selenium', 'silver', 'uranium'\n",
    "]\n",
    "\n",
    "print(\"\\nConverting feature columns to numeric...\")\n",
    "diab[feature_cols] = diab[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Converting 'is_safe' to numeric and dropping invalid rows...\")\n",
    "diab['is_safe'] = pd.to_numeric(diab['is_safe'], errors='coerce')\n",
    "before_drop = diab.shape[0]\n",
    "diab = diab.dropna(subset=['is_safe'])\n",
    "after_drop = diab.shape[0]\n",
    "print(f\"Dropped {before_drop - after_drop} rows with invalid 'is_safe' values.\")\n",
    "diab['is_safe'] = diab['is_safe'].astype(int)\n",
    "\n",
    "def impute_missing_values(dataframe):\n",
    "    print(\"\\nImputing zeros/NaNs in feature columns with median values...\")\n",
    "    for column in feature_cols:\n",
    "        zero_count = (dataframe[column] == 0).sum()\n",
    "        nan_count = dataframe[column].isna().sum()\n",
    "        if zero_count > 0 or nan_count > 0:\n",
    "            print(f\" - Column '{column}': {zero_count} zeros, {nan_count} NaNs before imputation\")\n",
    "        dataframe.loc[dataframe[column] == 0, column] = np.nan\n",
    "        dataframe[column].fillna(dataframe[column].median(), inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "diab = impute_missing_values(diab)\n",
    "print(\"Finished preprocessing. New dataset shape:\", diab.shape)\n",
    "print(\"\\nPreview of preprocessed data:\")\n",
    "print(diab.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa7bcf-ae76-4f87-adf5-f3c7d4bd7c1f",
   "metadata": {},
   "source": [
    "This code loads and preprocesses a water quality dataset. It reads the data from a CSV file, then prints the datasetâ€™s shape and info. It defines a list of feature columns and converts them to numbers, handling errors by coercing.\n",
    "\n",
    "The 'is_safe' column is also converted to numbers, and any rows with invalid data are removed. The number of dropped rows is shown. The 'is_safe' column is then changed to integers. A function replaces zeros and NaNs in the features with the median value of each column. After processing, it prints the new dataset shape and shows a preview of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4f8b6-9c23-4b5e-a0c7-4d69305e93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separating Dataset into Features and Output Label\n",
    "\n",
    "features = diab.iloc[:, :-1]\n",
    "labels = diab.iloc[:, -1]\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "print(\"\\nClass distribution (is_safe):\")\n",
    "print(labels.value_counts())\n",
    "positive_outcomes, negative_outcomes = labels.value_counts()\n",
    "total_samples = labels.count()\n",
    "print('\\n----------Checking for Data Imbalance------------')\n",
    "print('Number of Positive Outcomes: ', positive_outcomes)\n",
    "print('Percentage of Positive Outcomes: {}%'.format(round((positive_outcomes / total_samples) * 100, 2)))\n",
    "print('Number of Negative Outcomes : ', negative_outcomes)\n",
    "print('Percentage of Negative Outcomes: {}%'.format(round((negative_outcomes / total_samples) * 100, 2)))\n",
    "print('-------------------------------------------------\\n')\n",
    "\n",
    "sns.countplot(x=labels, label=\"Count\")\n",
    "plt.title(\"Class Distribution (is_safe) - Full Dataset\")\n",
    "plt.savefig(\"class_distribution_full.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f7b923-502c-4002-9d74-4c881b1f5be2",
   "metadata": {},
   "source": [
    "This code splits the dataset into features and labels. It uses iloc to select all columns except the last for features, and the last column for labels. It then prints their shapes.\n",
    "\n",
    "Next, it shows how many 'is_safe' labels are positive or negative. It calculates and displays the percentage of each to check for imbalance. A seaborn count plot is created to show the class distribution visually, and saved as \"class_distribution_full.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b868e7-09a8-4526-a5a8-9ce3934b4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Checking Correlation Between Attributes\n",
    "\n",
    "correlation_matrix = features.corr()\n",
    "fig, axis = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, linewidths=.5, fmt='.2f', ax=axis)\n",
    "plt.title(\"Correlation Heatmap (Features Only)\")\n",
    "plt.savefig(\"correlation_heatmap.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "full_corr = diab.corr()\n",
    "corr_with_target = full_corr['is_safe'].drop('is_safe')\n",
    "max_corr_feat = corr_with_target.abs().idxmax()\n",
    "print(\"\\nCorrelation of each feature with 'is_safe':\")\n",
    "print(corr_with_target.sort_values(ascending=False))\n",
    "print(f\"\\nHighest absolute correlation with 'is_safe': {max_corr_feat} \"\n",
    "      f\"(corr = {corr_with_target[max_corr_feat]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860481dd-c849-45b4-8488-f4fe3418ba11",
   "metadata": {},
   "source": [
    "This code checks how the features in the dataset are related. It calculates a correlation matrix for the feature columns and creates a heatmap to visualize these relationships. The heatmap is saved as \"correlation_heatmap.png\".\n",
    "\n",
    "Then, it finds the correlation of each feature with the target label 'is_safe', excluding 'is_safe' itself. It identifies and prints the feature that has the strongest absolute correlation with 'is_safe', along with the correlation values for all features in order from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da54f1-71f3-485e-a39d-e8de4cd9458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualizing Distributions (Histograms) & Symmetry\n",
    "\n",
    "features.hist(figsize=(10, 10))\n",
    "plt.suptitle(\"Histograms of Features\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_histograms.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Histograms plotted for all feature columns.\")\n",
    "\n",
    "skewness = features.skew()\n",
    "print(\"\\nSkewness of each feature:\")\n",
    "print(skewness.sort_values())\n",
    "\n",
    "sym_threshold = 0.5\n",
    "approx_symmetric = skewness[skewness.abs() < sym_threshold].index.tolist()\n",
    "print(f\"\\nFeatures with approximate symmetry (|skew| < {sym_threshold}):\")\n",
    "print(approx_symmetric if approx_symmetric else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64b6ba-20b6-431f-b68c-30411c7adb68",
   "metadata": {},
   "source": [
    "This code creates histograms for each feature to show their distributions. The histograms are arranged in a 10x10 grid and saved as \"feature_histograms.png\". A message is printed to confirm the plots are created.\n",
    "\n",
    "Next, it calculates the skewness of each feature to check how symmetric their distributions are. The skewness values are printed from smallest to largest. Features with skewness close to zero (less than 0.5 in absolute value) are considered roughly symmetric, and these features are identified and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3d285-a485-4214-b9b3-7f6b44fcafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pairwise Relationships (Pairplot)\n",
    "\n",
    "sns.pairplot(diab, hue='is_safe')\n",
    "plt.suptitle(\"Pairplot of Features Colored by is_safe\", y=1.02)\n",
    "plt.savefig(\"pairplot_is_safe.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Pairplot generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63caa89-1d7e-4424-99ac-3e67d657d459",
   "metadata": {},
   "source": [
    "This code creates a pairplot to show the relationships between all features in the dataset. The points are colored based on the 'is_safe' label. The plot is saved as \"pairplot_is_safe.png\". A message is printed to confirm the pairplot has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87ddc8-1f27-4fa8-aebe-d1105823a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train-Test Split and Normalization (70/30 Split)\n",
    "\n",
    "features_train_all, features_test_all, labels_train_all, labels_test_all = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "for dataset in [features_train_all, features_test_all, labels_train_all, labels_test_all]:\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training set shape:\", features_train_all.shape)\n",
    "print(\"Test set shape:\", features_test_all.shape)\n",
    "\n",
    "features_train_all_std = (features_train_all - features_train_all.mean()) / features_train_all.std()\n",
    "features_test_all_std = (features_test_all - features_test_all.mean()) / features_test_all.std()\n",
    "\n",
    "print(\"\\nTraining data (standardized) summary:\")\n",
    "print(features_train_all_std.describe())\n",
    "\n",
    "n_features = features_train_all_std.shape[1]\n",
    "print(\"Number of features (for LSTM):\", n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63802a03-e439-4905-b680-a7efa03a7261",
   "metadata": {},
   "source": [
    "This code splits the dataset into training and testing sets, with 70% for training and 30% for testing. It uses train_test_split with stratification to keep class proportions the same. The indices of both sets are reset for consistency.\n",
    "\n",
    "Then, it standardizes the features by subtracting the mean and dividing by the standard deviation for both training and test data. It prints a summary of the standardized training data and shows the number of features used for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56644eb0-a2ce-4d61-b3cd-d1938c1ce782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Visualizing Effect of SMOTE on Training Data\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "print(\"\\nOriginal training label distribution:\")\n",
    "orig_counts = Counter(labels_train_all)\n",
    "print(orig_counts)\n",
    "\n",
    "# Apply SMOTE to the training data only\n",
    "features_train_bal_all, labels_train_bal_all = smote.fit_resample(features_train_all_std, labels_train_all)\n",
    "\n",
    "print(\"\\nBalanced training label distribution after SMOTE:\")\n",
    "bal_counts = Counter(labels_train_bal_all)\n",
    "print(bal_counts)\n",
    "\n",
    "# Print class distribution before and after SMOTE\n",
    "print(\"\\nClass distribution before and after SMOTE:\")\n",
    "print(f\"Original training set class distribution: {orig_counts}\")\n",
    "print(f\"Balanced training set class distribution after SMOTE: {bal_counts}\")\n",
    "\n",
    "# Visualize the effect of SMOTE\n",
    "smote_df = pd.DataFrame({\n",
    "    'Class': ['0', '1', '0', '1'],\n",
    "    'Count': [\n",
    "        orig_counts.get(0, 0), orig_counts.get(1, 0),\n",
    "        bal_counts.get(0, 0), bal_counts.get(1, 0)\n",
    "    ],\n",
    "    'Dataset': ['Original Train', 'Original Train',\n",
    "                'SMOTE Train', 'SMOTE Train']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=smote_df, x='Class', y='Count', hue='Dataset')\n",
    "plt.title(\"Class Distribution Before and After SMOTE (Train Set)\")\n",
    "plt.savefig(\"smote_class_balance.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9a22a-4efc-4de3-a743-b80230bcbb22",
   "metadata": {},
   "source": [
    "This code shows the class distribution of the training labels before applying SMOTE, using print to display the counts. It then applies SMOTE to balance the classes by oversampling the minority class.\n",
    "\n",
    "After SMOTE, it prints the new class distribution and compares it to the original. A bar plot is created to visually compare the class balance before and after SMOTE, and it is saved as \"smote_class_balance.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cfa43-c93e-4a91-99c8-a43e2e9dc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Defining Metric Calculation and Model Evaluation Functions\n",
    "\n",
    "def calc_metrics(confusion_matrix_):\n",
    "    TP, FN = confusion_matrix_[0][0], confusion_matrix_[0][1]\n",
    "    FP, TN = confusion_matrix_[1][0], confusion_matrix_[1][1]\n",
    "\n",
    "    TPR = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    TNR = TN / (TN + FP) if TN + FP > 0 else 0\n",
    "    FPR = FP / (TN + FP) if TN + FP > 0 else 0\n",
    "    FNR = FN / (TP + FN) if TP + FN > 0 else 0\n",
    "    Precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    F1_measure = 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "    Accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0\n",
    "    Error_rate = 1 - Accuracy\n",
    "    BACC = (TPR + TNR) / 2\n",
    "    TSS = TPR - FPR\n",
    "    denom_hss = ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))\n",
    "    HSS = 2 * (TP * TN - FP * FN) / denom_hss if denom_hss != 0 else 0\n",
    "\n",
    "    metrics = [\n",
    "        TP, TN, FP, FN, TPR, TNR, FPR, FNR,\n",
    "        Precision, F1_measure, Accuracy, Error_rate,\n",
    "        BACC, TSS, HSS\n",
    "    ]\n",
    "    return metrics\n",
    "\n",
    "def get_metrics(model, features_train, features_test, labels_train, labels_test, LSTM_flag):\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    if LSTM_flag == 1:\n",
    "        Xtrain, Xtest, ytrain, ytest = map(\n",
    "            np.array, [features_train, features_test, labels_train, labels_test]\n",
    "        )\n",
    "\n",
    "        shape = Xtrain.shape\n",
    "        Xtrain_reshaped = Xtrain.reshape(len(Xtrain), shape[1], 1)\n",
    "        Xtest_reshaped = Xtest.reshape(len(Xtest), shape[1], 1)\n",
    "\n",
    "        model.fit(\n",
    "            Xtrain_reshaped, ytrain,\n",
    "            epochs=50,\n",
    "            validation_data=(Xtest_reshaped, ytest),\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        lstm_scores = model.evaluate(Xtest_reshaped, ytest, verbose=0)\n",
    "        predict_prob = model.predict(Xtest_reshaped, verbose=0)\n",
    "        pred_labels = (predict_prob > 0.5).astype(int)\n",
    "\n",
    "        matrix = confusion_matrix(ytest, pred_labels, labels=[1, 0])\n",
    "        brier = brier_score_loss(ytest, predict_prob)\n",
    "        roc_auc = roc_auc_score(ytest, predict_prob)\n",
    "\n",
    "        metrics.extend(calc_metrics(matrix))\n",
    "        metrics.extend([brier, roc_auc, lstm_scores[1]])\n",
    "\n",
    "    else:\n",
    "        model.fit(features_train, labels_train)\n",
    "        predicted = model.predict(features_test)\n",
    "        matrix = confusion_matrix(labels_test, predicted, labels=[1, 0])\n",
    "        proba = model.predict_proba(features_test)[:, 1]\n",
    "        brier = brier_score_loss(labels_test, proba)\n",
    "        roc_auc = roc_auc_score(labels_test, proba)\n",
    "\n",
    "        metrics.extend(calc_metrics(matrix))\n",
    "        metrics.extend([brier, roc_auc, model.score(features_test, labels_test)])\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a991c-b8b0-4c0a-9c2e-5514cf269ab1",
   "metadata": {},
   "source": [
    "This code defines two functions for evaluating a machine learning model. The first function, calc_metrics, takes a confusion matrix and computes various metrics like TPR, TNR, FPR, FNR, Precision, F1-score, Accuracy, Error rate, BACC, TSS, and HSS. These help measure how well the model performs in binary classification.\n",
    "\n",
    "The second function, get_metrics, evaluates a given model on training and test data. If using an LSTM model (indicated by LSTM_flag), it reshapes the data, trains the model for 50 epochs, and calculates the confusion matrix, Brier score, ROC AUC, and accuracy. For other models, it fits the model, makes predictions, and computes the same metrics. It returns a list of all these metrics for detailed performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa6463-a25a-4ada-90d7-12784cd031d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Hyperparameter Tuning (KNN & Random Forest)\n",
    "\n",
    "knn_parameters = {\"n_neighbors\": list(range(1, 16))}\n",
    "knn_model = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_cv = GridSearchCV(knn_model, knn_parameters, cv=10, n_jobs=-1)\n",
    "knn_cv.fit(features_train_all_std, labels_train_all)\n",
    "best_n_neighbors = knn_cv.best_params_[\"n_neighbors\"]\n",
    "print(\"Best KNN n_neighbors:\", best_n_neighbors)\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": list(range(10, 101, 10)),\n",
    "    \"min_samples_split\": [2, 4, 6, 8, 10]\n",
    "}\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_rf.fit(features_train_all_std, labels_train_all)\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "min_samples_split = best_rf_params[\"min_samples_split\"]\n",
    "n_estimators = best_rf_params[\"n_estimators\"]\n",
    "print(\"Best RF params:\", best_rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552aadd2-ae73-4902-b24b-8fca049da9a7",
   "metadata": {},
   "source": [
    "This code performs hyperparameter tuning for K-Nearest Neighbors (KNN) and Random Forest models using GridSearchCV. \n",
    "\n",
    "For KNN, it sets up a grid of n_neighbors values from 1 to 15. The model is trained with 10-fold cross-validation, and the optimal number of neighbors is identified via best_params_. The best value is then printed.\n",
    "\n",
    "For Random Forest, it creates a parameter grid to tune n_estimators from 10 to 100 in steps of 10, and min_samples_split from 2 to 10. The model is trained with 10-fold cross-validation to find the best parameter combination. The optimal parameters, including the number of estimators and minimum samples to split, are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442f1da-0bda-48ee-91b5-9e724feebd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 10-fold Stratified Cross-Validation with SMOTE\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "metric_columns = [\n",
    "    'TP', 'TN', 'FP', 'FN', 'TPR', 'TNR', 'FPR', 'FNR',\n",
    "    'Precision', 'F1_measure', 'Accuracy', 'Error_rate',\n",
    "    'BACC', 'TSS', 'HSS', 'Brier_score', 'AUC', 'Acc_by_package_fn'\n",
    "]\n",
    "\n",
    "knn_metrics_list = []\n",
    "rf_metrics_list = []\n",
    "lstm_metrics_list = []\n",
    "\n",
    "for iter_num, (train_index, test_index) in enumerate(\n",
    "    cv_stratified.split(features_train_all_std, labels_train_all), start=1\n",
    "):\n",
    "    print(f\"\\n--- Cross-Validation Iteration {iter_num} ---\")\n",
    "\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=best_n_neighbors, n_jobs=-1)\n",
    "    rf_model = RandomForestClassifier(\n",
    "        min_samples_split=min_samples_split,\n",
    "        n_estimators=n_estimators,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(\n",
    "        LSTM(\n",
    "            64,\n",
    "            activation='relu',\n",
    "            input_shape=(n_features, 1),\n",
    "            return_sequences=False\n",
    "        )\n",
    "    )\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "    lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    features_train = features_train_all_std.iloc[train_index, :]\n",
    "    features_test = features_train_all_std.iloc[test_index, :]\n",
    "    labels_train = labels_train_all[train_index]\n",
    "    labels_test = labels_train_all[test_index]\n",
    "\n",
    "    features_train_bal, labels_train_bal = smote.fit_resample(features_train, labels_train)\n",
    "\n",
    "    knn_metrics = get_metrics(knn_model, features_train_bal, features_test, labels_train_bal, labels_test, 0)\n",
    "    rf_metrics = get_metrics(rf_model, features_train_bal, features_test, labels_train_bal, labels_test, 0)\n",
    "    lstm_metrics = get_metrics(lstm_model, features_train_bal, features_test, labels_train_bal, labels_test, 1)\n",
    "\n",
    "    knn_metrics_list.append(knn_metrics)\n",
    "    rf_metrics_list.append(rf_metrics)\n",
    "    lstm_metrics_list.append(lstm_metrics)\n",
    "\n",
    "    iter_df = pd.DataFrame(\n",
    "        {\n",
    "            \"KNN\": knn_metrics,\n",
    "            \"RF\": rf_metrics,\n",
    "            \"LSTM\": lstm_metrics\n",
    "        },\n",
    "        index=metric_columns\n",
    "    )\n",
    "\n",
    "    print(f\"\\nIteration {iter_num}:\")\n",
    "    print(f\"----- Metrics for all Algorithms in Iteration {iter_num} -----\")\n",
    "    print(iter_df.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fb472-2058-4fec-8c52-76431ce49cd8",
   "metadata": {},
   "source": [
    "This code performs 10-fold stratified cross-validation with SMOTE to balance the training data. It uses StratifiedKFold to ensure each fold maintains the class distribution. In each fold, it splits the data into training and testing sets based on the generated indices.\n",
    "\n",
    "SMOTE is applied to the training data to address class imbalance. Then, it trains and evaluates three models KNN, Random Forest, and LSTM using the get_metrics function, storing their performance metrics in separate lists.\n",
    "\n",
    "After each fold, a DataFrame is created to display the metrics for all models, and the results are printed. These metrics include TPR, FPR, F1-score, AUC, and others, providing a comprehensive assessment of model performance across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bfcbe-5d73-4e42-a7d5-cbc0be50db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Average Performance & Per-Algorithm Iteration Tables\n",
    "\n",
    "metric_index_df = [\n",
    "    'iter1', 'iter2', 'iter3', 'iter4', 'iter5',\n",
    "    'iter6', 'iter7', 'iter8', 'iter9', 'iter10'\n",
    "]\n",
    "\n",
    "knn_metrics_df = pd.DataFrame(knn_metrics_list, columns=metric_columns, index=metric_index_df)\n",
    "rf_metrics_df = pd.DataFrame(rf_metrics_list, columns=metric_columns, index=metric_index_df)\n",
    "lstm_metrics_df = pd.DataFrame(lstm_metrics_list, columns=metric_columns, index=metric_index_df)\n",
    "\n",
    "print(\"\\nMetrics for Algorithm 1 (KNN):\")\n",
    "print(knn_metrics_df.T.round(2).to_string())\n",
    "\n",
    "print(\"\\nMetrics for Algorithm 2 (RF):\")\n",
    "print(rf_metrics_df.T.round(2).to_string())\n",
    "\n",
    "print(\"\\nMetrics for Algorithm 3 (LSTM):\")\n",
    "print(lstm_metrics_df.T.round(2).to_string())\n",
    "\n",
    "knn_avg_df = knn_metrics_df.mean()\n",
    "rf_avg_df = rf_metrics_df.mean()\n",
    "lstm_avg_df = lstm_metrics_df.mean()\n",
    "\n",
    "avg_performance_df = pd.DataFrame(\n",
    "    {\"KNN\": knn_avg_df, \"RF\": rf_avg_df, \"LSTM\": lstm_avg_df},\n",
    "    index=metric_columns\n",
    ")\n",
    "\n",
    "print(\"\\nAverage performance over 10 folds (rounded):\")\n",
    "print(avg_performance_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1969d45-2610-4462-a4d9-845a7831c542",
   "metadata": {},
   "source": [
    "This code calculates and displays the performance metrics for the KNN, RF, and LSTM models over 10-fold cross-validation. It organizes the metrics for each fold into DataFrames, with each row representing an iteration and columns showing different evaluation metrics.\n",
    "\n",
    "The metrics for each model are printed separately, showing how each performed in each fold. Then, it computes the average performance across all folds by averaging each column in the metric DataFrame. These averages are stored in a new DataFrame and printed, offering a summarized view of each model's overall performance. The results are rounded to three decimal places for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af512b6-6238-43b0-8c7c-3229d1bd19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Final ROC Curves on Held-out Test Set\n",
    "\n",
    "print(\"\\nTraining final KNN on balanced data and plotting ROC...\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_n_neighbors, n_jobs=-1)\n",
    "knn_model.fit(features_train_bal_all, labels_train_bal_all)\n",
    "y_score = knn_model.predict_proba(features_test_all_std)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(labels_test_all, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, label=f\"KNN AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"KNN ROC Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"roc_knn.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"KNN Test AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nTraining final Random Forest on balanced data and plotting ROC...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    min_samples_split=min_samples_split,\n",
    "    n_estimators=n_estimators,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(features_train_bal_all, labels_train_bal_all)\n",
    "y_score_rf = rf_model.predict_proba(features_test_all_std)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(labels_test_all, y_score_rf)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "plt.figure()\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"RF AUC = {roc_auc_rf:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"Random Forest ROC Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"roc_rf.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Random Forest Test AUC: {roc_auc_rf:.4f}\")\n",
    "\n",
    "print(\"\\nTraining final LSTM on balanced data and plotting ROC...\")\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(\n",
    "    LSTM(\n",
    "        64,\n",
    "        activation='relu',\n",
    "        input_shape=(n_features, 1),\n",
    "        return_sequences=False\n",
    "    )\n",
    ")\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train_array = np.array(features_train_bal_all)\n",
    "X_test_array = features_test_all_std.to_numpy()\n",
    "y_train_array = np.array(labels_train_bal_all)\n",
    "y_test_array = labels_test_all.to_numpy()\n",
    "\n",
    "input_train = X_train_array.reshape(len(X_train_array), n_features, 1)\n",
    "input_test = X_test_array.reshape(len(X_test_array), n_features, 1)\n",
    "\n",
    "lstm_model.fit(input_train, y_train_array, epochs=50,\n",
    "               validation_data=(input_test, y_test_array), verbose=0)\n",
    "\n",
    "predict_lstm = lstm_model.predict(input_test, verbose=0)\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(labels_test_all, predict_lstm)\n",
    "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_lstm, tpr_lstm, label=f\"LSTM AUC = {roc_auc_lstm:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"LSTM ROC Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"roc_lstm.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"LSTM Test AUC: {roc_auc_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62975b08-d006-4be9-9ba4-68bba6307617",
   "metadata": {},
   "source": [
    "This code trains and evaluates the KNN, Random Forest, and LSTM models on the balanced training data, plotting their ROC curves and computing the AUC for each.\n",
    "\n",
    "For KNN, the model is trained on the balanced data, and predicted probabilities are used to calculate the FPR and TPR for the ROC curve. The AUC is computed, and the ROC curve is saved as \"roc_knn.png\". The AUC value is printed.\n",
    "\n",
    "Similarly, the Random Forest model is trained on the balanced data, with its ROC curve plotted and saved as \"roc_rf.png\", and its AUC printed.\n",
    "\n",
    "Finally, the LSTM model is trained on the reshaped balanced data. Its ROC curve is plotted and saved as \"roc_lstm.png\", and the AUC is printed. Each model's performance is summarized by their AUC scores, reflecting their prediction quality on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
